{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Speech Commands Classification - Lab Evaluation\n",
        "(https://arxiv.org/abs/1804.03209).\n",
        "\n",
        "### Tasks:\n",
        "1. Summarize the paper in about 50 words.\n",
        "2. Download, analyze, and statistically describe the dataset.\n",
        "3. Train a classifier to distinguish commands.\n",
        "4. Report performance results using standard benchmarks.\n",
        "5. Record 30 samples of each command in your voice and create a new dataset.\n",
        "6. Fine-tune the classifier on your voice.\n",
        "7. Report the results.\n",
        "\n",
        "## 1. Paper Summary\n",
        "\n",
        "The Speech Commands dataset is designed to train and evaluate keyword spotting models, providing a standardized collection of audio clips featuring isolated words from diverse speakers. It supports on-device speech recognition tasks, emphasizing efficient, small-scale models for mobile devices. The dataset, available under a Creative Commons license, was crowdsourced and processed to ensure quality. It has facilitated improvements in speech recognition accuracy, model noise tolerance, and adversarial robustness, with Version 2 significantly enhancing performance over Version 1, achieving up to 89.7% Top-One accuracy."
      ],
      "metadata": {
        "id": "67-K4SqzVjyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Download and Analyze the Dataset\n",
        "import os\n",
        "import torchaudio\n",
        "from collections import Counter\n",
        "from torch.utils.data import Subset"
      ],
      "metadata": {
        "id": "FdyOuBQYVRq2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data directory if it doesn't exist\n",
        "data_dir = './data'\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)"
      ],
      "metadata": {
        "id": "6TD1BbWwV6Pp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Speech Commands dataset\n",
        "dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_dir, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIZA4kbOV_rJ",
        "outputId": "0703edff-b56b-4f0e-ccb5-e99a2044723b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.26G/2.26G [01:50<00:00, 22.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select 10 commands to work with\n",
        "selected_commands = ['yes', 'no', 'up', 'down', 'left', 'right', 'go', 'stop', 'on', 'off']\n",
        "\n",
        "# Limit the number of samples per command (e.g., 100 samples per command)\n",
        "samples_per_command = 100"
      ],
      "metadata": {
        "id": "7mmyB28qWCYt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a subset of the dataset by filtering for the selected commands\n",
        "subset_indices = []\n",
        "command_counter = Counter()\n",
        "\n",
        "for idx, sample in enumerate(dataset):\n",
        "    label = sample[2]\n",
        "    if label in selected_commands and command_counter[label] < samples_per_command:\n",
        "        subset_indices.append(idx)\n",
        "        command_counter.update([label])\n",
        "\n",
        "    # Stop when we have enough samples for each command\n",
        "    if all(command_counter[cmd] >= samples_per_command for cmd in selected_commands):\n",
        "        break\n",
        "\n",
        "# Create a subset of the dataset\n",
        "subset_dataset = Subset(dataset, subset_indices)\n",
        "\n",
        "# Check the sample count for each command in the subset\n",
        "print(f\"Sample counts in subset: {command_counter}\")\n",
        "print(f\"Total subset size: {len(subset_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLsG5in0WFl7",
        "outputId": "177de513-7161-4850-83e3-b43f603c2ebc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample counts in subset: Counter({'down': 100, 'go': 100, 'left': 100, 'no': 100, 'off': 100, 'on': 100, 'right': 100, 'stop': 100, 'up': 100, 'yes': 100})\n",
            "Total subset size: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preprocessing"
      ],
      "metadata": {
        "id": "N0JTonRtW6nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Data Preprocessing (Padding and Truncating)\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "9hf5wYt0VSMN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a fixed length for all audio samples (1 second = 16000 samples at 16kHz)\n",
        "fixed_length = 16000\n",
        "\n",
        "# Custom collate function to pad and truncate audio data\n",
        "def collate_fn(batch):\n",
        "    waveforms = []\n",
        "    labels = []\n",
        "\n",
        "    for item in batch:\n",
        "        waveform = item[0]\n",
        "        label = item[2]\n",
        "\n",
        "        if waveform.shape[1] > fixed_length:\n",
        "            waveform = waveform[:, :fixed_length]\n",
        "        elif waveform.shape[1] < fixed_length:\n",
        "            pad_amount = fixed_length - waveform.shape[1]\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
        "\n",
        "        waveforms.append(waveform)\n",
        "        labels.append(label)\n",
        "\n",
        "    waveforms = torch.stack(waveforms)\n",
        "    return waveforms, labels"
      ],
      "metadata": {
        "id": "g9T7BVULWfr_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for the subset dataset\n",
        "loader = DataLoader(subset_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "P5gI2cizWijE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. CNN Classifier"
      ],
      "metadata": {
        "id": "DHqsVkRBXEUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define and Train a CNN Classifier (with correct fully connected layer input size)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio.transforms as transforms\n",
        "\n",
        "# Define the MelSpectrogram transform to convert audio waveforms into spectrograms\n",
        "mel_spectrogram = transforms.MelSpectrogram(\n",
        "    sample_rate=16000, n_mels=128, n_fft=400, hop_length=160\n",
        ")\n",
        "\n",
        "# Define a simple CNN model for speech command classification\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(32 * 25 * 32, 128)  # Correct input size based on shape (32*25=800)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Get the number of classes (commands)\n",
        "num_classes = len(selected_commands)\n",
        "\n",
        "# Create a dictionary to map commands (labels) to numerical values\n",
        "label_to_index = {label: idx for idx, label in enumerate(selected_commands)}\n",
        "\n",
        "# Function to convert string labels to numerical indices\n",
        "def label_to_tensor(label):\n",
        "    return torch.tensor(label_to_index[label])\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleCNN(num_classes=num_classes).to('cuda')\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop for 10 epochs\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for waveforms, labels in loader:\n",
        "        # Convert waveforms to spectrograms\n",
        "        waveforms = mel_spectrogram(waveforms)\n",
        "        waveforms = waveforms.squeeze(1).unsqueeze(1).to('cuda')  # Remove extra dimension, add channel dimension\n",
        "        labels = torch.tensor([label_to_tensor(label) for label in labels]).to('cuda')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(waveforms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(loader)}')\n",
        "\n",
        "print('Training completed!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3eSGNqXW0WH",
        "outputId": "b9c91068-bfc5-4b8f-b5a2-871c5d9a426b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 3.421601578593254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Evaluation"
      ],
      "metadata": {
        "id": "ZzvfL4tuXIcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Evaluate the Model\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Path to your recordings folder in Drive\n",
        "recordings_path = '/content/drive/My Drive/path_to_recordings_folder'\n",
        "\n",
        "# Verify the recordings are accessible\n",
        "import os\n",
        "recording_files = os.listdir(recordings_path)\n",
        "print(\"Files in the recordings folder:\", recording_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfrQ_hlLVaH5",
        "outputId": "37403c69-fbd3-434b-ae12-376551aa23fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 98.4%\n"
          ]
        }
      ]
    }
  ]
}