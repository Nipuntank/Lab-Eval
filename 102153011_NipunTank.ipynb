{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabhmeharbedi/102165002-SESS_LE1/blob/main/102165002_PRABHMEHAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1: Introduction and Paper Summary\n",
        "\n",
        "# Speech Commands Classification - Lab Evaluation\n",
        "\n",
        "This notebook follows the evaluation tasks based on the paper\n",
        "[Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition]\n",
        "(https://arxiv.org/abs/1804.03209).\n",
        "\n",
        "### Tasks:\n",
        "1. Summarize the paper in about 50 words.\n",
        "2. Download, analyze, and statistically describe the dataset.\n",
        "3. Train a classifier to distinguish commands.\n",
        "4. Report performance results using standard benchmarks.\n",
        "5. Record 30 samples of each command in your voice and create a new dataset.\n",
        "6. Fine-tune the classifier on your voice.\n",
        "7. Report the results.\n",
        "\n",
        "## 1. Paper Summary\n",
        "\n",
        "The Speech Commands dataset is designed to train and evaluate keyword spotting models, providing a standardized collection of audio clips featuring isolated words from diverse speakers. It supports on-device speech recognition tasks, emphasizing efficient, small-scale models for mobile devices. The dataset, available under a Creative Commons license, was crowdsourced and processed to ensure quality. It has facilitated improvements in speech recognition accuracy, model noise tolerance, and adversarial robustness, with Version 2 significantly enhancing performance over Version 1, achieving up to 89.7% Top-One accuracy."
      ],
      "metadata": {
        "id": "67-K4SqzVjyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZsARhtADn183",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a38595-ecff-4d46-a391-8094f57edb15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Analyze Dataset"
      ],
      "metadata": {
        "id": "qsVDXSTdWLZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Download and Analyze the Dataset\n",
        "import os\n",
        "import torchaudio\n",
        "from collections import Counter\n",
        "from torch.utils.data import Subset"
      ],
      "metadata": {
        "id": "FdyOuBQYVRq2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data directory if it doesn't exist\n",
        "data_dir = './data'\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)"
      ],
      "metadata": {
        "id": "6TD1BbWwV6Pp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Speech Commands dataset\n",
        "dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_dir, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIZA4kbOV_rJ",
        "outputId": "4019b2c1-462c-4dec-f2bb-d7a8e7fdac40"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.26G/2.26G [01:14<00:00, 32.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select 10 commands to work with\n",
        "selected_commands = ['yes', 'no', 'up', 'down', 'left', 'right', 'go', 'stop', 'on', 'off']\n",
        "\n",
        "# Limit the number of samples per command (e.g., 100 samples per command)\n",
        "samples_per_command = 100"
      ],
      "metadata": {
        "id": "7mmyB28qWCYt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a subset of the dataset by filtering for the selected commands\n",
        "subset_indices = []\n",
        "command_counter = Counter()\n",
        "\n",
        "for idx, sample in enumerate(dataset):\n",
        "    label = sample[2]\n",
        "    if label in selected_commands and command_counter[label] < samples_per_command:\n",
        "        subset_indices.append(idx)\n",
        "        command_counter.update([label])\n",
        "\n",
        "    # Stop when we have enough samples for each command\n",
        "    if all(command_counter[cmd] >= samples_per_command for cmd in selected_commands):\n",
        "        break\n",
        "\n",
        "# Create a subset of the dataset\n",
        "subset_dataset = Subset(dataset, subset_indices)\n",
        "\n",
        "# Check the sample count for each command in the subset\n",
        "print(f\"Sample counts in subset: {command_counter}\")\n",
        "print(f\"Total subset size: {len(subset_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLsG5in0WFl7",
        "outputId": "3c071701-5016-4182-aa86-b80221d5c92d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample counts in subset: Counter({'down': 100, 'go': 100, 'left': 100, 'no': 100, 'off': 100, 'on': 100, 'right': 100, 'stop': 100, 'up': 100, 'yes': 100})\n",
            "Total subset size: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preprocessing"
      ],
      "metadata": {
        "id": "N0JTonRtW6nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Data Preprocessing (Padding and Truncating)\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "9hf5wYt0VSMN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a fixed length for all audio samples (1 second = 16000 samples at 16kHz)\n",
        "fixed_length = 16000\n",
        "\n",
        "# Custom collate function to pad and truncate audio data\n",
        "def collate_fn(batch):\n",
        "    waveforms = []\n",
        "    labels = []\n",
        "\n",
        "    for item in batch:\n",
        "        waveform = item[0]\n",
        "        label = item[2]\n",
        "\n",
        "        if waveform.shape[1] > fixed_length:\n",
        "            waveform = waveform[:, :fixed_length]\n",
        "        elif waveform.shape[1] < fixed_length:\n",
        "            pad_amount = fixed_length - waveform.shape[1]\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
        "\n",
        "        waveforms.append(waveform)\n",
        "        labels.append(label)\n",
        "\n",
        "    waveforms = torch.stack(waveforms)\n",
        "    return waveforms, labels"
      ],
      "metadata": {
        "id": "g9T7BVULWfr_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for the subset dataset\n",
        "loader = DataLoader(subset_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "P5gI2cizWijE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. CNN Classifier"
      ],
      "metadata": {
        "id": "DHqsVkRBXEUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define and Train a CNN Classifier (with correct fully connected layer input size)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio.transforms as transforms\n",
        "\n",
        "# Define the MelSpectrogram transform to convert audio waveforms into spectrograms\n",
        "mel_spectrogram = transforms.MelSpectrogram(\n",
        "    sample_rate=16000, n_mels=128, n_fft=400, hop_length=160\n",
        ")\n",
        "\n",
        "# Define a simple CNN model for speech command classification\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(32 * 25 * 32, 128)  # Correct input size based on shape (32*25=800)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Get the number of classes (commands)\n",
        "num_classes = len(selected_commands)\n",
        "\n",
        "# Create a dictionary to map commands (labels) to numerical values\n",
        "label_to_index = {label: idx for idx, label in enumerate(selected_commands)}\n",
        "\n",
        "# Function to convert string labels to numerical indices\n",
        "def label_to_tensor(label):\n",
        "    return torch.tensor(label_to_index[label])\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleCNN(num_classes=num_classes).to('cuda')\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop for 10 epochs\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for waveforms, labels in loader:\n",
        "        # Convert waveforms to spectrograms\n",
        "        waveforms = mel_spectrogram(waveforms)\n",
        "        waveforms = waveforms.squeeze(1).unsqueeze(1).to('cuda')  # Remove extra dimension, add channel dimension\n",
        "        labels = torch.tensor([label_to_tensor(label) for label in labels]).to('cuda')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(waveforms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(loader)}')\n",
        "\n",
        "print('Training completed!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3eSGNqXW0WH",
        "outputId": "014ced98-c4c8-4b85-e8c3-3974b12f9501"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 2.735433742403984\n",
            "Epoch 2/50, Loss: 1.7791217155754566\n",
            "Epoch 3/50, Loss: 1.5099541135132313\n",
            "Epoch 4/50, Loss: 1.256291102617979\n",
            "Epoch 5/50, Loss: 1.0833818912506104\n",
            "Epoch 6/50, Loss: 0.9143548309803009\n",
            "Epoch 7/50, Loss: 0.8382234964519739\n",
            "Epoch 8/50, Loss: 0.9330774340778589\n",
            "Epoch 9/50, Loss: 0.7458871146664023\n",
            "Epoch 10/50, Loss: 0.6382007179781795\n",
            "Epoch 11/50, Loss: 0.5344852702692151\n",
            "Epoch 12/50, Loss: 0.5043157776817679\n",
            "Epoch 13/50, Loss: 0.4485340788960457\n",
            "Epoch 14/50, Loss: 0.40360586065799\n",
            "Epoch 15/50, Loss: 0.38896756747271866\n",
            "Epoch 16/50, Loss: 0.3600412090308964\n",
            "Epoch 17/50, Loss: 0.32153833215124905\n",
            "Epoch 18/50, Loss: 0.33028649259358644\n",
            "Epoch 19/50, Loss: 0.4526979032671079\n",
            "Epoch 20/50, Loss: 0.50119389872998\n",
            "Epoch 21/50, Loss: 0.39286120724864304\n",
            "Epoch 22/50, Loss: 0.292071697069332\n",
            "Epoch 23/50, Loss: 0.22603586642071605\n",
            "Epoch 24/50, Loss: 0.18977399868890643\n",
            "Epoch 25/50, Loss: 0.1651693576714024\n",
            "Epoch 26/50, Loss: 0.14813494449481368\n",
            "Epoch 27/50, Loss: 0.14083219354506582\n",
            "Epoch 28/50, Loss: 0.12521097087301314\n",
            "Epoch 29/50, Loss: 0.11918085382785648\n",
            "Epoch 30/50, Loss: 0.10722422157414258\n",
            "Epoch 31/50, Loss: 0.09465410909615457\n",
            "Epoch 32/50, Loss: 0.09277340536937118\n",
            "Epoch 33/50, Loss: 0.14291622454766184\n",
            "Epoch 34/50, Loss: 0.13897527661174536\n",
            "Epoch 35/50, Loss: 0.4364656560937874\n",
            "Epoch 36/50, Loss: 0.3020824936684221\n",
            "Epoch 37/50, Loss: 0.20608075696509331\n",
            "Epoch 38/50, Loss: 0.1481175790540874\n",
            "Epoch 39/50, Loss: 0.13439146149903536\n",
            "Epoch 40/50, Loss: 0.10547472047619522\n",
            "Epoch 41/50, Loss: 0.1021641586557962\n",
            "Epoch 42/50, Loss: 0.08191418595379218\n",
            "Epoch 43/50, Loss: 0.09978851026971824\n",
            "Epoch 44/50, Loss: 0.069928436059854\n",
            "Epoch 45/50, Loss: 0.06442617130232975\n",
            "Epoch 46/50, Loss: 0.06061381663312204\n",
            "Epoch 47/50, Loss: 0.05973472725600004\n",
            "Epoch 48/50, Loss: 0.05308408767450601\n",
            "Epoch 49/50, Loss: 0.05270414277765667\n",
            "Epoch 50/50, Loss: 0.048306178039638326\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Evaluation"
      ],
      "metadata": {
        "id": "ZzvfL4tuXIcl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfrQ_hlLVaH5",
        "outputId": "b67c3d81-0617-4c4f-a4a4-847ae88c7004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Files in the recordings folder: ['no_22.wav', 'yes_1.wav', 'yes_19.wav', 'no_1.wav', 'yes_27.wav', 'yes_4.wav', 'yes_14.wav', 'no_15.wav', 'yes_24.wav', 'yes_22.wav', 'yes_20.wav', 'yes_18.wav', 'yes_6.wav', 'no_16.wav', 'no_27.wav', 'no_19.wav', 'no_5.wav', 'yes_10.wav', 'yes_25.wav', 'no_14.wav', 'yes_8.wav', 'no_3.wav', 'yes_28.wav', 'no_8.wav', 'yes_2.wav', 'no_12.wav', 'yes_17.wav', 'yes_12.wav', 'no_4.wav', 'yes_23.wav', 'yes_21.wav', 'yes_9.wav', 'no_28.wav', 'no_2.wav', 'no_21.wav', 'yes_7.wav', 'yes_13.wav', 'yes_29.wav', 'yes_30.wav', 'no_26.wav', 'yes_3.wav', 'no_11.wav', 'no_24.wav', 'no_23.wav', 'no_20.wav', 'no_7.wav', 'yes_26.wav', 'yes_5.wav', 'yes_16.wav', 'no_29.wav', 'no_30.wav', 'no_6.wav', 'yes_11.wav', 'no_25.wav', 'no_10.wav', 'no_13.wav', 'yes_15.wav', 'no_17.wav', 'no_9.wav', 'no_18.wav', 'down_6.wav', 'down_1.wav', 'down_30.wav', 'down_29.wav', 'down_28.wav', 'down_27.wav', 'down_26.wav', 'down_25.wav', 'down_23.wav', 'down_24.wav', 'down_22.wav', 'down_21.wav', 'down_20.wav', 'down_19.wav', 'down_17.wav', 'down_12.wav', 'down_16.wav', 'down_14.wav', 'down_15.wav', 'down_13.wav', 'down_18.wav', 'down_11.wav', 'down_10.wav', 'down_9.wav', 'down_8.wav', 'down_7.wav', 'down_2.wav', 'down_4.wav', 'down_3.wav', 'down_5.wav', 'left_12.wav', 'left_11.wav', 'left_10.wav', 'left_9.wav', 'left_8.wav', 'left_7.wav', 'left_6.wav', 'left_3.wav', 'left_2.wav', 'left_4.wav', 'left_5.wav', 'left_1.wav', 'up_10.wav', 'up_9.wav', 'up_8.wav', 'up_3.wav', 'up_6.wav', 'up_2.wav', 'up_4.wav', 'up_5.wav', 'up_1.wav', 'up_7.wav', 'up_1 copy 4.wav', 'left_30.wav', 'left_28.wav', 'left_27.wav', 'left_29.wav', 'left_26.wav', 'left_25.wav', 'left_22.wav', 'left_23.wav', 'left_24.wav', 'left_21.wav', 'left_20.wav', 'left_19.wav', 'left_14.wav', 'left_17.wav', 'left_16.wav', 'left_15.wav', 'left_18.wav', 'left_13.wav', 'up_30.wav', 'up_29.wav', 'up_28.wav', 'up_26.wav', 'up_23.wav', 'up_24.wav', 'up_25.wav', 'up_22.wav', 'up_27.wav', 'up_21.wav', 'up_19.wav', 'up_18.wav', 'up_17.wav', 'up_15.wav', 'up_16.wav', 'up_14.wav', 'up_13.wav', 'up_11.wav', 'up_12.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Recording"
      ],
      "metadata": {
        "id": "Yq4v5vANXOoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Install sounddevice and Record 30 Samples of Your Voice\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5Snqlf2Va43",
        "outputId": "59832f35-5674-4dad-ba9c-fc9f33fb4044"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the recordings folder in Drive\n",
        "recordings_path = '/content/drive/MyDrive/speech/Recordings'\n",
        "\n",
        "# Verify the recordings are accessible\n",
        "try:\n",
        "    recording_files = os.listdir(recordings_path)\n",
        "    print(\"Recordings found:\")\n",
        "    for file in recording_files:\n",
        "        print(file)\n",
        "except FileNotFoundError:\n",
        "    print(f\"The folder {recordings_path} does not exist.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtbrJq8oOrAL",
        "outputId": "00838a1f-86c9-4122-b37c-c05cf768be62"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "The folder /content/drive/MyDrive/speech/Recordings does not exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check if all files exist before loading\n",
        "def verify_file_paths(file_list, label):\n",
        "    missing_files = [file for file in file_list if not os.path.isfile(file)]\n",
        "    if missing_files:\n",
        "        print(f\"Missing {label} files:\", missing_files)\n",
        "    else:\n",
        "        print(f\"All {label} files are present.\")\n",
        "\n",
        "# Verify files for each label\n",
        "verify_file_paths(yes_files, \"yes\")\n",
        "verify_file_paths(no_files, \"no\")\n",
        "verify_file_paths(up_files, \"up\")\n",
        "verify_file_paths(down_files, \"down\")\n",
        "verify_file_paths(left_files, \"left\")"
      ],
      "metadata": {
        "id": "TsTyXUauRWgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E3gFTmqgRWT3"
      }
    }
  ]
}